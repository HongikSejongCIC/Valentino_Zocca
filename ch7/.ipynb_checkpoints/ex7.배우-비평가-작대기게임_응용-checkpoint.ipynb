{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import deque\n",
    "\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "ACTIONS_COUNT = 2\n",
    "FUTURE_REWARD_DISCOUNT = 0.9\n",
    "LEARN_RATE_ACTOR = 0.01\n",
    "LEARN_RATE_CRITIC = 0.01\n",
    "STORE_SCORES_LEN = 5\n",
    "GAMES_PER_TRAINING = 3\n",
    "INPUT_NODES = env.observation_space.shape[0]\n",
    "\n",
    "ACTOR_HIDDEN = 20\n",
    "CRITIC_HIDDEN = 20\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "actor_feed_forward_weights_1 = tf.Variable(tf.truncated_normal([INPUT_NODES, ACTOR_HIDDEN], stddev=0.01))\n",
    "actor_feed_forward_bias_1 = tf.Variable(tf.constant(0.0, shape=[ACTOR_HIDDEN]))\n",
    "\n",
    "actor_feed_forward_weights_2 = tf.Variable(tf.truncated_normal([ACTOR_HIDDEN, ACTIONS_COUNT], stddev=0.01))\n",
    "actor_feed_forward_bias_2 = tf.Variable(tf.constant(0.1, shape=[ACTIONS_COUNT]))\n",
    "\n",
    "actor_input_placeholder = tf.placeholder(\"float\", [None, INPUT_NODES])\n",
    "actor_hidden_layer = tf.nn.tanh(\n",
    "    tf.matmul(actor_input_placeholder, actor_feed_forward_weights_1) + actor_feed_forward_bias_1)\n",
    "actor_output_layer = tf.nn.softmax(\n",
    "    tf.matmul(actor_hidden_layer, actor_feed_forward_weights_2) + actor_feed_forward_bias_2)\n",
    "\n",
    "actor_action_placeholder = tf.placeholder(\"float\", [None, ACTIONS_COUNT])\n",
    "actor_advantage_placeholder = tf.placeholder(\"float\", [None, 1])\n",
    "\n",
    "policy_gradient = tf.reduce_mean(actor_advantage_placeholder * actor_action_placeholder * tf.log(actor_output_layer))\n",
    "actor_train_operation = tf.train.AdamOptimizer(LEARN_RATE_ACTOR).minimize(-policy_gradient)\n",
    "\n",
    "critic_feed_forward_weights_1 = tf.Variable(tf.truncated_normal([INPUT_NODES, CRITIC_HIDDEN], stddev=0.01))\n",
    "critic_feed_forward_bias_1 = tf.Variable(tf.constant(0.0, shape=[CRITIC_HIDDEN]))\n",
    "\n",
    "critic_feed_forward_weights_2 = tf.Variable(tf.truncated_normal([CRITIC_HIDDEN, 1], stddev=0.01))\n",
    "critic_feed_forward_bias_2 = tf.Variable(tf.constant(0.0, shape=[1]))\n",
    "\n",
    "critic_input_placeholder = tf.placeholder(\"float\", [None, INPUT_NODES])\n",
    "critic_hidden_layer = tf.nn.tanh(\n",
    "    tf.matmul(critic_input_placeholder, critic_feed_forward_weights_1) + critic_feed_forward_bias_1)\n",
    "critic_output_layer = tf.matmul(critic_hidden_layer, critic_feed_forward_weights_2) + critic_feed_forward_bias_2\n",
    "\n",
    "critic_target_placeholder = tf.placeholder(\"float\", [None, 1])\n",
    "\n",
    "critic_cost = tf.reduce_mean(tf.square(critic_target_placeholder - critic_output_layer))\n",
    "critic_train_operation = tf.train.AdamOptimizer(LEARN_RATE_CRITIC).minimize(critic_cost)\n",
    "\n",
    "critic_baseline = critic_target_placeholder - critic_output_layer\n",
    "\n",
    "scores = deque(maxlen=STORE_SCORES_LEN)\n",
    "\n",
    "# 첫 번째 액션에서는 아무것도 하지않도록 설정 \n",
    "last_action = np.zeros(ACTIONS_COUNT)\n",
    "last_action[1] = 1\n",
    "\n",
    "time = 0\n",
    "\n",
    "session.run(tf.initialize_all_variables())\n",
    "\n",
    "\n",
    "def choose_next_action(state):\n",
    "    probability_of_actions = session.run(actor_output_layer, feed_dict={actor_input_placeholder: [state]})[0]\n",
    "    try:\n",
    "        move = np.random.multinomial(1, probability_of_actions)\n",
    "    except ValueError:\n",
    "        # probability_of_actions 의 합이 1보다 커지면 반올림 에러가 발생할 수도 있다. \n",
    "        # 그래서 유효한 값으로 값을 조금 줄인다. \n",
    "        move = np.random.multinomial(1, probability_of_actions / (sum(probability_of_actions) + 1e-6))\n",
    "    return move\n",
    "\n",
    "\n",
    "def train(states, actions_taken, advantages):\n",
    "    # 리워드를 얻게 해줄 지금 상태 내의 액션을 학습한다. \n",
    "    session.run(actor_train_operation, feed_dict={\n",
    "        actor_input_placeholder: states,\n",
    "        actor_action_placeholder: actions_taken,\n",
    "        actor_advantage_placeholder: advantages})\n",
    "\n",
    "\n",
    "last_state = env.reset()\n",
    "total_reward = 0\n",
    "current_game_observations = []\n",
    "current_game_rewards = []\n",
    "current_game_actions = []\n",
    "\n",
    "episode_observation = []\n",
    "episode_rewards = []\n",
    "episode_actions = []\n",
    "games = 0\n",
    "plot_x = []\n",
    "plot_y = []\n",
    "\n",
    "critic_costs = deque(maxlen=10)\n",
    "\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    last_action = choose_next_action(last_state)\n",
    "    current_state, reward, terminal, info = env.step(np.argmax(last_action))\n",
    "    total_reward += reward\n",
    "\n",
    "    if terminal:\n",
    "        reward = -.10\n",
    "    else:\n",
    "        reward = 0.1\n",
    "\n",
    "    current_game_observations.append(last_state)\n",
    "    current_game_rewards.append(reward)\n",
    "    current_game_actions.append(last_action)\n",
    "\n",
    "    if terminal:\n",
    "        games += 1\n",
    "        scores.append(total_reward)\n",
    "\n",
    "        if games % STORE_SCORES_LEN == 0:\n",
    "            plot_x.append(games)\n",
    "            plot_y.append(np.mean(scores))\n",
    "\n",
    "        # 비평가를 위한 일시적으로 사용하는 값을 가져온다. \n",
    "        cumulative_reward = 0\n",
    "        for i in reversed(range(len(current_game_observations))):\n",
    "            cumulative_reward = current_game_rewards[i] + FUTURE_REWARD_DISCOUNT * cumulative_reward\n",
    "            current_game_rewards[i] = [cumulative_reward]\n",
    "\n",
    "        values_t = session.run(critic_output_layer, {\n",
    "            critic_input_placeholder: current_game_observations})\n",
    "        advantages = []\n",
    "\n",
    "        for i in range(len(current_game_observations) - 1):\n",
    "            advantages.append([current_game_rewards[i][0] + FUTURE_REWARD_DISCOUNT*values_t[i+1][0] - values_t[i][0]])\n",
    "\n",
    "        advantages.append([current_game_rewards[-1][0]-values_t[-1][0]])\n",
    "\n",
    "        _, cost = session.run([critic_train_operation, critic_cost], {\n",
    "                    critic_input_placeholder: current_game_observations,\n",
    "                    critic_target_placeholder: current_game_rewards})\n",
    "\n",
    "        critic_costs.append(cost)\n",
    "\n",
    "        print(\"게임: %s 리워드: %s 평균 점수: %s 비평가 비용: %s\" %\n",
    "              (games, total_reward,\n",
    "               np.mean(scores), np.mean(critic_costs)))\n",
    "\n",
    "        episode_observation.extend(current_game_observations)\n",
    "        episode_actions.extend(current_game_actions)\n",
    "        episode_rewards.extend(advantages)\n",
    "\n",
    "        total_reward = 0\n",
    "        current_game_observations = []\n",
    "        current_game_rewards = []\n",
    "        current_game_actions = []\n",
    "\n",
    "        if games % GAMES_PER_TRAINING == 0:\n",
    "            episode_rewards = np.array(episode_rewards)\n",
    "            normalized_rewards = episode_rewards - np.mean(episode_rewards)\n",
    "            normalized_rewards /= np.std(normalized_rewards)\n",
    "\n",
    "            train(episode_observation, episode_actions, normalized_rewards)\n",
    "\n",
    "            episode_observation = []\n",
    "            episode_actions = []\n",
    "            episode_rewards = []\n",
    "\n",
    "    time += 1\n",
    "\n",
    "    # 기존 값을 업데이트한다. \n",
    "    if terminal:\n",
    "        last_state = env.reset()\n",
    "    else:\n",
    "        last_state = current_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
