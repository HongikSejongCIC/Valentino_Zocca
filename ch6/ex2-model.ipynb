{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Train for 500\n",
      "Total iters: 500\n",
      "데이터 길이:  3200403\n",
      "char_list:  97 ['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'À', 'à', 'á', 'â', 'ä', 'ç', 'è', 'é', 'ê', 'ë', 'í', 'î', 'ï', 'ó', 'ô', 'ú', 'ý', '—', '…']\n",
      "char_dict:  {'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, '*': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '=': 24, '?': 25, 'A': 26, 'B': 27, 'C': 28, 'D': 29, 'E': 30, 'F': 31, 'G': 32, 'H': 33, 'I': 34, 'J': 35, 'K': 36, 'L': 37, 'M': 38, 'N': 39, 'O': 40, 'P': 41, 'Q': 42, 'R': 43, 'S': 44, 'T': 45, 'U': 46, 'V': 47, 'W': 48, 'X': 49, 'Y': 50, 'Z': 51, 'a': 52, 'b': 53, 'c': 54, 'd': 55, 'e': 56, 'f': 57, 'g': 58, 'h': 59, 'i': 60, 'j': 61, 'k': 62, 'l': 63, 'm': 64, 'n': 65, 'o': 66, 'p': 67, 'q': 68, 'r': 69, 's': 70, 't': 71, 'u': 72, 'v': 73, 'w': 74, 'x': 75, 'y': 76, 'z': 77, 'À': 78, 'à': 79, 'á': 80, 'â': 81, 'ä': 82, 'ç': 83, 'è': 84, 'é': 85, 'ê': 86, 'ë': 87, 'í': 88, 'î': 89, 'ï': 90, 'ó': 91, 'ô': 92, 'ú': 93, 'ý': 94, '—': 95, '…': 96}\n",
      "labels:  ['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'À', 'à', 'á', 'â', 'ä', 'ç', 'è', 'é', 'ê', 'ë', 'í', 'î', 'ï', 'ó', 'ô', 'ú', 'ý', '—', '…']\n",
      "i:  50\n",
      "loss: 3.1092920303344727 (162.74227666854858 sec.)\n",
      "i:  100\n",
      "loss: 3.079944372177124 (168.53720808029175 sec.)\n",
      "Reset initial state\n",
      "i:  150\n",
      "loss: 2.8905773162841797 (177.15889286994934 sec.)\n",
      "i:  200\n",
      "loss: 2.5553128719329834 (160.99547696113586 sec.)\n",
      "Reset initial state\n",
      "i:  250\n",
      "loss: 2.3660449981689453 (159.41413688659668 sec.)\n",
      "i:  300\n",
      "loss: 2.2681217193603516 (165.5317599773407 sec.)\n",
      "Reset initial state\n",
      "i:  350\n",
      "loss: 2.1888015270233154 (162.5400252342224 sec.)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-25d73e457ea9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-25d73e457ea9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n\\nTrain for {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total iters: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0mtrain_and_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mtotal_iterations\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-25d73e457ea9>\u001b[0m in \u001b[0;36mtrain_and_sample\u001b[0;34m(minibatch_iterations, restore)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 feed_dict={\n\u001b[0;32m--> 167\u001b[0;31m                     model.inputs: input_batch, model.targets: target_batch})\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import time\n",
    "import codecs\n",
    "import locale\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import data_reader\n",
    "\n",
    "class Model(object):\n",
    "    \"\"\"RNN 언어모델\"\"\"\n",
    "    def __init__(self, batch_size, sequence_length, lstm_sizes, dropout,\n",
    "                 labels, save_path):\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.lstm_sizes = lstm_sizes\n",
    "        self.labels = labels\n",
    "        self.label_map = {val: idx for idx, val in enumerate(labels)}\n",
    "        self.number_of_characters = len(labels)\n",
    "        self.save_path = save_path\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def init_graph(self):\n",
    "        # 변수 시퀀스 길이 \n",
    "        self.inputs = tf.placeholder(\n",
    "            tf.int32, [self.batch_size, self.sequence_length])\n",
    "        self.targets = tf.placeholder(\n",
    "            tf.int32, [self.batch_size, self.sequence_length])\n",
    "        self.init_architecture()\n",
    "        self.saver = tf.train.Saver(tf.trainable_variables())\n",
    "\n",
    "    def init_architecture(self):\n",
    "    \n",
    "        # 멀티레이어 LSTM 셀을 정의한다. \n",
    "        self.one_hot_inputs = tf.one_hot(\n",
    "            self.inputs, depth=self.number_of_characters)\n",
    "        cell_list = [tf.nn.rnn_cell.LSTMCell(lstm_size, state_is_tuple=True)\n",
    "                     for lstm_size in self.lstm_sizes]\n",
    "        self.multi_cell_lstm = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            cell_list, state_is_tuple=True)\n",
    "        \n",
    "        # LSTM 메모리의 상태 초기화\n",
    "        # 배치간에 사용하기위헤서 그래프 메모리안의 상태를 유지한다. \n",
    "        self.initial_state = self.multi_cell_lstm.zero_state(\n",
    "            self.batch_size, tf.float32)\n",
    "        \n",
    "        # 변수들을 변환해서 배치 간에 상태를 저장할 수 있도록 만든다. \n",
    "        # LSTM 상태는 텐서에 튜플로 구성되어있고, \n",
    "        # 이 구조는 LSTM상태로 사용하기 위해 재가공이 필요하다. \n",
    "        \n",
    "        self.state_variables = tf.contrib.framework.nest.pack_sequence_as(\n",
    "            self.initial_state,\n",
    "            [tf.Variable(var, trainable=False)\n",
    "             for var in tf.contrib.framework.nest.flatten(self.initial_state)])\n",
    "\n",
    "        # 시간에 따른 rnn을 정의한다. \n",
    "        lstm_output, final_state = tf.nn.dynamic_rnn(\n",
    "            cell=self.multi_cell_lstm, inputs=self.one_hot_inputs,\n",
    "            initial_state=self.state_variables)\n",
    "        \n",
    "        # 결과를 반환하기 전에 다음 배치에 적용하기 위한 새로운 상태를 \n",
    "        # 초기 상태로 강제로 만든다. \n",
    "        store_states = [\n",
    "            state_variable.assign(new_state)\n",
    "            for (state_variable, new_state) in zip(\n",
    "                tf.contrib.framework.nest.flatten(self.state_variables),\n",
    "                tf.contrib.framework.nest.flatten(final_state))]\n",
    "        with tf.control_dependencies(store_states):\n",
    "            lstm_output = tf.identity(lstm_output)\n",
    "            \n",
    "        # 모든 결과값에 선형변환을 적용할 수 있도록 재가공한다. \n",
    "        output_flat = tf.reshape(lstm_output, (-1, self.lstm_sizes[-1]))\n",
    "        \n",
    "        # 결과 레이어 정의 \n",
    "        self.logit_weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                (self.lstm_sizes[-1], self.number_of_characters), stddev=0.01),\n",
    "            name='logit_weights')\n",
    "        self.logit_bias = tf.Variable(\n",
    "            tf.zeros((self.number_of_characters)), name='logit_bias')\n",
    "        \n",
    "        # 마지막 레이어에 변환 적용 \n",
    "        self.logits_flat = tf.matmul(\n",
    "            output_flat, self.logit_weights) + self.logit_bias\n",
    "        probabilities_flat = tf.nn.softmax(self.logits_flat)\n",
    "        self.probabilities = tf.reshape(\n",
    "            probabilities_flat,\n",
    "            (self.batch_size, -1, self.number_of_characters))\n",
    "\n",
    "    def init_train_op(self, optimizer):\n",
    "        # 단조로운 logits과 호환가능하도록 타겟을 단조롭게 만들기 \n",
    "        targets_flat = tf.reshape(self.targets, (-1, ))\n",
    "        \n",
    "        # 모든 결과값에 대한 loss\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=self.logits_flat, labels=targets_flat, name='x_entropy'\n",
    "        )\n",
    "        self.loss = tf.reduce_mean(loss)\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        gradients = tf.gradients(loss, trainable_variables)\n",
    "        gradients, _ = tf.clip_by_global_norm(\n",
    "            gradients, 5)\n",
    "        self.train_op = optimizer.apply_gradients(\n",
    "            zip(gradients, trainable_variables))\n",
    "\n",
    "    def sample(self, session, prime_string, sample_length):\n",
    "        self.reset_state(session)\n",
    "        # 초기 상태(prime state)\n",
    "        print('초기 string: ', prime_string)\n",
    "        for character in prime_string:\n",
    "            character_idx = self.label_map[character]\n",
    "            out = session.run(\n",
    "                self.probabilities,\n",
    "                feed_dict={self.inputs: np.asarray([[character_idx]])})\n",
    "            sample_label = np.random.choice(\n",
    "                self.labels, size=(1),  p=out[0, 0])\n",
    "        output_sample = prime_string\n",
    "        print('샘플링 시작')\n",
    "        \n",
    "        # sample_length 횟수만큼 샘플링 \n",
    "        for _ in range(sample_length):\n",
    "            sample_label = np.random.choice(\n",
    "                self.labels, size=(1),  p=out[0, 0])[0]\n",
    "            output_sample += sample_label\n",
    "            sample_idx = self.label_map[sample_label]\n",
    "            out = session.run(\n",
    "                self.probabilities,\n",
    "                feed_dict={self.inputs: np.asarray([[sample_idx]])})\n",
    "        return output_sample\n",
    "\n",
    "    def reset_state(self, session):\n",
    "        for state in tf.contrib.framework.nest.flatten(self.state_variables):\n",
    "            session.run(state.initializer)\n",
    "\n",
    "    def save(self, sess):\n",
    "        self.saver.save(sess, self.save_path)\n",
    "\n",
    "    def restore(self, sess):\n",
    "        self.saver.restore(sess, self.save_path)\n",
    "\n",
    "\n",
    "def train_and_sample(minibatch_iterations, restore):\n",
    "    tf.reset_default_graph()\n",
    "    batch_size = 64\n",
    "    lstm_sizes = [512, 512]\n",
    "    batch_len = 100\n",
    "    learning_rate = 2e-3\n",
    "\n",
    "    filepath = './wap.txt'\n",
    "\n",
    "    data_feed = data_reader.DataReader(\n",
    "         filepath, batch_len, batch_size)\n",
    "    labels = data_feed.char_list\n",
    "    print('labels: ', labels)\n",
    "\n",
    "    save_path = './model.tf'\n",
    "    model = Model(\n",
    "        batch_size, batch_len, lstm_sizes, 0.8, labels,\n",
    "        save_path)\n",
    "    model.init_graph()\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    model.init_train_op(optimizer)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        if restore:\n",
    "            print('모델을 다시 복원합니다.')\n",
    "            model.restore(sess)\n",
    "        model.reset_state(sess)\n",
    "        start_time = time.time()\n",
    "        for i in range(minibatch_iterations):\n",
    "            input_batch, target_batch = next(iter(data_feed))\n",
    "            loss, _ = sess.run(\n",
    "                [model.loss, model.train_op],\n",
    "                feed_dict={\n",
    "                    model.inputs: input_batch, model.targets: target_batch})\n",
    "            if i % 50 == 0 and i != 0:\n",
    "                print('i: ', i)\n",
    "                duration = time.time() - start_time\n",
    "                print('loss: {} ({} sec.)'.format(loss, duration))\n",
    "                start_time = time.time()\n",
    "            if i % 1000 == 0 and i != 0:\n",
    "                model.save(sess)\n",
    "            if i % 100 == 0 and i != 0:\n",
    "                print('Reset initial state')\n",
    "                model.reset_state(sess)\n",
    "            if i % 1000 == 0 and i != 0:\n",
    "                print('Reset minibatch feeder')\n",
    "                data_feed.reset_indices()\n",
    "        model.save(sess)\n",
    "\n",
    "    print('\\n {}번 반복 이후의 샘플링'.format(minibatch_iterations))\n",
    "    tf.reset_default_graph()\n",
    "    model = Model(\n",
    "        1, None, lstm_sizes, 1.0, labels, save_path)\n",
    "    model.init_graph()\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        model.restore(sess)\n",
    "        print('\\nSample 1:')\n",
    "        sample = model.sample(\n",
    "            sess, prime_string=u'\\n\\nThis feeling was ', sample_length=500)\n",
    "        print(u'sample: \\n{}'.format(sample))\n",
    "        print('\\nSample 2:')\n",
    "        sample = model.sample(\n",
    "            sess, prime_string=u'She was born in the year ', sample_length=500)\n",
    "        print(u'sample: \\n{}'.format(sample))\n",
    "        print('\\nSample 3:')\n",
    "        sample = model.sample(\n",
    "            sess, prime_string=u'The meaning of this all is ',\n",
    "            sample_length=500)\n",
    "        print(u'sample: \\n{}'.format(sample))\n",
    "        print('\\nSample 4:')\n",
    "        sample = model.sample(\n",
    "            sess,\n",
    "            prime_string=u'In the midst of a conversation on political matters Anna Pávlovna burst out:,',\n",
    "            sample_length=500)\n",
    "        print(u'sample: \\n{}'.format(sample))\n",
    "        print('\\nSample 5:')\n",
    "        sample = model.sample(\n",
    "            sess, prime_string=u'\\n\\nCHAPTER X\\n\\n',\n",
    "            sample_length=500)\n",
    "        print(u'sample: \\n{}'.format(sample))\n",
    "        print('\\nSample 5:')\n",
    "        sample = model.sample(\n",
    "            sess, prime_string=u'\"If only you knew,\"',\n",
    "            sample_length=500)\n",
    "        print(u'sample: \\n{}'.format(sample))\n",
    "\n",
    "\n",
    "def main():\n",
    "    total_iterations = 500\n",
    "    print('\\n\\n\\nTrain for {}'.format(500))\n",
    "    print('Total iters: {}'.format(total_iterations))\n",
    "    train_and_sample(500, restore=False)\n",
    "    for i in [500, 1000, 3000, 5000, 10000, 30000, 50000, 100000, 300000]:\n",
    "        total_iterations += i\n",
    "        print('\\n\\n\\nTrain for {}'.format(i))\n",
    "        print('Total iters: {}'.format(total_iterations))\n",
    "        train_and_sample(i, restore=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
