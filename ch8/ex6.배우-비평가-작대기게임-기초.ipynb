{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac88/code/.virtualenvs/deeplearning/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mac88/code/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Game: 1 reward 40.0 average scores 40.0 critic cost 63.666405\n",
      "Game: 2 reward 13.0 average scores 26.5 critic cost 43.160557\n",
      "Game: 3 reward 13.0 average scores 22.0 critic cost 36.289856\n",
      "Game: 4 reward 21.0 average scores 21.75 critic cost 37.002453\n",
      "Game: 5 reward 20.0 average scores 21.4 critic cost 37.011913\n",
      "Game: 6 reward 61.0 average scores 25.6 critic cost 43.256153\n",
      "Game: 7 reward 15.0 average scores 26.0 critic cost 40.8274\n",
      "Game: 8 reward 24.0 average scores 28.2 critic cost 41.10932\n",
      "Game: 9 reward 14.0 average scores 26.8 critic cost 39.21635\n",
      "Game: 10 reward 45.0 average scores 31.8 critic cost 41.728706\n",
      "Game: 11 reward 18.0 average scores 23.2 critic cost 40.764072\n",
      "Game: 12 reward 10.0 average scores 22.2 critic cost 38.481888\n",
      "Game: 13 reward 12.0 average scores 19.8 critic cost 36.864967\n",
      "Game: 14 reward 20.0 average scores 21.0 critic cost 36.672405\n",
      "Game: 15 reward 40.0 average scores 20.0 critic cost 38.01871\n",
      "Game: 16 reward 23.0 average scores 21.0 critic cost 37.94749\n",
      "Game: 17 reward 22.0 average scores 23.4 critic cost 37.70172\n",
      "Game: 18 reward 48.0 average scores 30.6 critic cost 38.965714\n",
      "Game: 19 reward 16.0 average scores 29.8 critic cost 38.198215\n",
      "Game: 20 reward 13.0 average scores 24.4 critic cost 37.061745\n",
      "Game: 21 reward 25.0 average scores 24.8 critic cost 37.08946\n",
      "Game: 22 reward 10.0 average scores 22.4 critic cost 35.809727\n",
      "Game: 23 reward 50.0 average scores 22.8 critic cost 36.589012\n",
      "Game: 24 reward 21.0 average scores 23.8 critic cost 36.159157\n",
      "Game: 25 reward 23.0 average scores 25.8 critic cost 35.82901\n",
      "Game: 26 reward 49.0 average scores 30.6 critic cost 36.370728\n",
      "Game: 27 reward 63.0 average scores 41.2 critic cost 36.9799\n",
      "Game: 28 reward 50.0 average scores 41.2 critic cost 37.33689\n",
      "Game: 29 reward 20.0 average scores 41.0 critic cost 36.700066\n",
      "Game: 30 reward 31.0 average scores 42.6 critic cost 36.565945\n",
      "Game: 31 reward 13.0 average scores 35.4 critic cost 35.7351\n",
      "Game: 32 reward 40.0 average scores 30.8 critic cost 35.723885\n",
      "Game: 33 reward 54.0 average scores 31.6 critic cost 35.909428\n",
      "Game: 34 reward 58.0 average scores 39.2 critic cost 36.073574\n",
      "Game: 35 reward 20.0 average scores 37.0 critic cost 35.43093\n",
      "Game: 36 reward 20.0 average scores 38.4 critic cost 34.853085\n",
      "Game: 37 reward 35.0 average scores 37.4 critic cost 34.59239\n",
      "Game: 38 reward 33.0 average scores 33.2 critic cost 34.20388\n",
      "Game: 39 reward 18.0 average scores 25.2 critic cost 33.5347\n",
      "Game: 40 reward 19.0 average scores 25.0 critic cost 32.955994\n",
      "Game: 41 reward 11.0 average scores 23.2 critic cost 32.34358\n",
      "Game: 42 reward 43.0 average scores 24.8 critic cost 32.0445\n",
      "Game: 43 reward 21.0 average scores 22.4 critic cost 31.534918\n",
      "Game: 44 reward 14.0 average scores 21.6 critic cost 30.99771\n",
      "Game: 45 reward 16.0 average scores 21.0 critic cost 30.50248\n",
      "Game: 46 reward 21.0 average scores 23.0 critic cost 30.028477\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-01255f028b32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0mlast_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoose_next_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/.virtualenvs/deeplearning/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/.virtualenvs/deeplearning/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/.virtualenvs/deeplearning/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/.virtualenvs/deeplearning/lib/python3.6/site-packages/pyglet/window/cocoa/__init__.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_mouse_cursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/.virtualenvs/deeplearning/lib/python3.6/site-packages/pyglet/gl/cocoa.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nscontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflushBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/.virtualenvs/deeplearning/lib/python3.6/site-packages/pyglet/libs/darwin/cocoapy/runtime.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;34m\"\"\"Call the method with the given arguments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;31m######################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/.virtualenvs/deeplearning/lib/python3.6/site-packages/pyglet/libs/darwin/cocoapy/runtime.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, objc_id, *args)\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m             \u001b[0;31m# Convert result to python type if it is a instance or class pointer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mObjCInstance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import deque\n",
    "\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "ACTIONS_COUNT = 2\n",
    "FUTURE_REWARD_DISCOUNT = 0.9\n",
    "LEARN_RATE_ACTOR = 0.01\n",
    "LEARN_RATE_CRITIC = 0.01\n",
    "STORE_SCORES_LEN = 5\n",
    "GAMES_PER_TRAINING = 3\n",
    "INPUT_NODES = env.observation_space.shape[0]\n",
    "\n",
    "ACTOR_HIDDEN = 20\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "actor_feed_forward_weights_1 = tf.Variable(tf.truncated_normal([INPUT_NODES, ACTOR_HIDDEN], stddev=0.01))\n",
    "actor_feed_forward_bias_1 = tf.Variable(tf.constant(0.0, shape=[ACTOR_HIDDEN]))\n",
    "\n",
    "actor_feed_forward_weights_2 = tf.Variable(tf.truncated_normal([ACTOR_HIDDEN, ACTIONS_COUNT], stddev=0.01))\n",
    "actor_feed_forward_bias_2 = tf.Variable(tf.constant(0.1, shape=[ACTIONS_COUNT]))\n",
    "\n",
    "actor_input_placeholder = tf.placeholder(\"float\", [None, INPUT_NODES])\n",
    "actor_hidden_layer = tf.nn.tanh(\n",
    "    tf.matmul(actor_input_placeholder, actor_feed_forward_weights_1) + actor_feed_forward_bias_1)\n",
    "actor_output_layer = tf.nn.softmax(\n",
    "    tf.matmul(actor_hidden_layer, actor_feed_forward_weights_2) + actor_feed_forward_bias_2)\n",
    "\n",
    "actor_action_placeholder = tf.placeholder(\"float\", [None, ACTIONS_COUNT])\n",
    "actor_advantage_placeholder = tf.placeholder(\"float\", [None, 1])\n",
    "\n",
    "policy_gradient = tf.reduce_mean(actor_advantage_placeholder * actor_action_placeholder * tf.log(actor_output_layer))\n",
    "actor_train_operation = tf.train.AdamOptimizer(LEARN_RATE_ACTOR).minimize(-policy_gradient)\n",
    "\n",
    "CRITIC_HIDDEN = 20\n",
    "\n",
    "critic_feed_forward_weights_1 = tf.Variable(tf.truncated_normal([INPUT_NODES, CRITIC_HIDDEN], stddev=0.01))\n",
    "critic_feed_forward_bias_1 = tf.Variable(tf.constant(0.0, shape=[CRITIC_HIDDEN]))\n",
    "\n",
    "critic_feed_forward_weights_2 = tf.Variable(tf.truncated_normal([CRITIC_HIDDEN, 1], stddev=0.01))\n",
    "critic_feed_forward_bias_2 = tf.Variable(tf.constant(0.0, shape=[1]))\n",
    "\n",
    "critic_input_placeholder = tf.placeholder(\"float\", [None, INPUT_NODES])\n",
    "critic_hidden_layer = tf.nn.tanh(\n",
    "    tf.matmul(critic_input_placeholder, critic_feed_forward_weights_1) + critic_feed_forward_bias_1)\n",
    "critic_output_layer = tf.matmul(critic_hidden_layer, critic_feed_forward_weights_2) + critic_feed_forward_bias_2\n",
    "\n",
    "critic_target_placeholder = tf.placeholder(\"float\", [None, 1])\n",
    "\n",
    "critic_cost = tf.reduce_mean(tf.square(critic_target_placeholder - critic_output_layer))\n",
    "critic_train_operation = tf.train.AdamOptimizer(LEARN_RATE_CRITIC).minimize(critic_cost)\n",
    "\n",
    "critic_advantages = critic_target_placeholder - critic_output_layer\n",
    "\n",
    "scores = deque(maxlen=STORE_SCORES_LEN)\n",
    "\n",
    "# 첫 번째 액션에서는 아무것도 하지않도록 설정 \n",
    "last_action = np.zeros(ACTIONS_COUNT)\n",
    "last_action[1] = 1\n",
    "\n",
    "time = 0\n",
    "\n",
    "session.run(tf.initialize_all_variables())\n",
    "\n",
    "\n",
    "def choose_next_action(state):\n",
    "    probability_of_actions = session.run(actor_output_layer, feed_dict={actor_input_placeholder: [state]})[0]\n",
    "    try:\n",
    "        move = np.random.multinomial(1, probability_of_actions)\n",
    "    except ValueError:\n",
    "        # probability_of_actions 의 합이 1보다 커지면 반올림 에러가 발생할 수도 있다. \n",
    "        # 그래서 유효한 값으로 값을 조금 줄인다. \n",
    "        move = np.random.multinomial(1, probability_of_actions / (sum(probability_of_actions) + 1e-6))\n",
    "    return move\n",
    "\n",
    "\n",
    "def train(states, actions_taken, advantages):\n",
    "    \n",
    "    # 리워드를 얻게 해줄 지금 상태 내의 액션을 학습한다. \n",
    "    session.run(actor_train_operation, feed_dict={\n",
    "        actor_input_placeholder: states,\n",
    "        actor_action_placeholder: actions_taken,\n",
    "        actor_advantage_placeholder: advantages})\n",
    "\n",
    "\n",
    "last_state = env.reset()\n",
    "total_reward = 0\n",
    "current_game_observations = []\n",
    "current_game_rewards = []\n",
    "current_game_actions = []\n",
    "\n",
    "episode_observation = []\n",
    "episode_rewards = []\n",
    "episode_actions = []\n",
    "games = 0\n",
    "\n",
    "critic_costs = deque(maxlen=100)\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    last_action = choose_next_action(last_state)\n",
    "    current_state, reward, terminal, info = env.step(np.argmax(last_action))\n",
    "    total_reward += reward\n",
    "\n",
    "    if terminal:\n",
    "        reward = -.10\n",
    "\n",
    "    current_game_observations.append(last_state)\n",
    "    current_game_rewards.append(reward)\n",
    "    current_game_actions.append(last_action)\n",
    "\n",
    "    if terminal:\n",
    "        games += 1\n",
    "        scores.append(total_reward)\n",
    "\n",
    "        # get temporal difference values for critic\n",
    "        # 비평가를 위한 일시적으로 사용하는 값을 가져온다. \n",
    "        cumulative_reward = 0\n",
    "        for i in reversed(range(len(current_game_observations))):\n",
    "            cumulative_reward = current_game_rewards[i] + FUTURE_REWARD_DISCOUNT * cumulative_reward\n",
    "            current_game_rewards[i] = [cumulative_reward]\n",
    "\n",
    "        _, cost, advantages = session.run([critic_train_operation, critic_cost, critic_advantages], {\n",
    "            critic_input_placeholder: current_game_observations,\n",
    "            critic_target_placeholder: current_game_rewards})\n",
    "\n",
    "        critic_costs.append(cost)\n",
    "\n",
    "        print(\"게임: %s 리워드: %s 평균 점수: %s 비평가 비용: %s\" %\n",
    "              (games, total_reward,\n",
    "               np.mean(scores), np.mean(critic_costs)))\n",
    "\n",
    "        episode_observation.extend(current_game_observations)\n",
    "        episode_actions.extend(current_game_actions)\n",
    "        episode_rewards.extend(advantages)\n",
    "\n",
    "        total_reward = 0\n",
    "        current_game_observations = []\n",
    "        current_game_rewards = []\n",
    "        current_game_actions = []\n",
    "\n",
    "        if games % GAMES_PER_TRAINING == 0:\n",
    "            train(episode_observation, episode_actions, episode_rewards)\n",
    "\n",
    "            episode_observation = []\n",
    "            episode_actions = []\n",
    "            episode_rewards = []\n",
    "\n",
    "    time += 1\n",
    "\n",
    "    # 기존 값을 업데이트한다. \n",
    "    if terminal:\n",
    "        last_state = env.reset()\n",
    "    else:\n",
    "        last_state = current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
